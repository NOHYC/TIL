# 신경망

## 신경망 요소

> 1. 네트워크( 또는 모델 )를 구성하는 층
> 2. 입력 데이터와 그에 상응하는 타깃
> 3. 학습 데이터에 사용할 피드백 신호를 정의하는 손실함수
> 4. 학습 진행 방식을 결정하는 옵티마이저

- 연속된 층으로 구성된 네트워크가 입력데이터를 예측으로 매핑합니다. 손실함수는 예측과 타깃을 비교하여 네트워크의 예측이 기댓값에 얼마나 잘 맞는지를 측정하는 손실 값을 만듭니다. 옵티마이저는 손실값을 사용하여 네트워크 가중치를 업데이트합니다.



### 딥러닝의 구성 단위 ( 층 )

1. 2D 텐서( samples, features )가 저장된 간단한 벡터 데이터는 완전 연결층 ( fully connected layer)이나 밀집층이라고도 불리는 밀집 연결 층에 의해 처리 ==> ( 케라스에서 Dense 클래스 )

2. 3D 텐서( samples, timesteps, features)로 저장된 시퀸스 데이터는 보통 LSTM 같은 순환층에 의해 처리

1. 4D 텐서( samples, height, width, channels )로 저장된 이미지 데이터는 일반적으로 2D 합성곱(convolution layer)에 의해 처리됩니다. ( 케라스에서 Conv2D 클래스 )

> 케라스에서는 호환 가능한 층들을 엮어 데이터 변환 파이프라인( pipeline )을 구성하므로서 딥러닝 모델을 만든다. 여기서 층 호환성 ( layer compatibility )은 각 층이 특정 크기의 입력 텐서만 받고 특정 크기의 출력 텐서를 반환하는 것



### 층의 네트워크 (모델)

- 딥러닝 모델은 층으로 만든 비순환 유향 그래프( Directed Acyclic Graph, DAG) ex) 하나의 입력을 하나의 출력으로 매핑하는 층을 순서대로 쌓는 것

- 자주 보게될 네트워크 구조
  - 가지( Branch )가 2개인 네트워크
  - 출력이 여러 개인 네트워크
  - 인셉션( Inception ) 블록

> 네트워크 구조는 가설 공간( Hypothesis space)을 정의
>
> 네트워크 구조를 선택함으로써 가능성 있는 공간(가설 공간)을 입력 데이터에서 출력 데이터로 매핑하는 일련의 특정 텐서 연산으로 제한
>
> 이런 텐서 연산에 포함된 가중치 텐서의 좋은 값을 찾는 것



### 학습 과정을 조절하는 열쇠 ( 손실함수와 옵티마이저 )

- 손실함수(loss function) ( 목적함수 (objective function)) : 훈련하는 동안 최소화될 값
- 옵티마이저 (optimizer) : 손실함수를 기반으로 네트워크가 어떻게 업데이트 될 지 결정 ( 특정 종류의 확률적 경사 하강법(SGD) 구현 )

> 여러 개의 출력을 내는 신경망은 여러 개의 손실함수를 가질 수 있다. 하지만 경사 하강법 과정은 하나의 스칼라 손실값을 기준으로 함. 따라서 손실이 여러 개인 네트워크에서는 모든 손실을 평균 내서 하나의 스칼라 양으로 합쳐짐.

- 문제에 맞는 올바른 손실함수를 선택하는 것이 중요 => 네트워크가 손실을 최소화하기 위해서 편법을 사용할 수도 있다. 

  - 분류, 회귀와 시퀸스 예측 같은 일반적인 문제에서는 올바른 손실함수를 선택하는 간단한 지침이 있다.
  - 2개의 클래스가 있는 분류 문제에는 이진 크로스 엔트로피
  - 여러 개의 클래스가 있는 분류 문제에는 범주형 크로스 엔트로피
  - 회귀 문제에는 평균 제곱 오차
  - 시퀸스 학습 문제에는 CTC
  - 완전히 새로운 연구를 할 때만 독자적 손실함수를 만들게 됨

  